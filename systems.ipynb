{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e509e265-3d91-483c-a68c-26112d93be98",
   "metadata": {},
   "source": [
    "# Benchmarking & Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c065f20e-c9f3-403e-a3db-0fb2660491af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "from torch.profiler import ProfilerActivity\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    " \n",
    "def run_operation1(dim: int, operation: Callable) -> Callable:\n",
    "    # Setup: create one random dim x dim matrices\n",
    "    x = torch.randn(dim, dim, device=get_device())\n",
    "    # Return a function to perform the operation\n",
    "    return lambda : operation(x)\n",
    "\n",
    "def run_operation2(dim: int, operation: Callable) -> Callable:\n",
    "    # Setup: create two random dim x dim matrices\n",
    "    x = torch.randn(dim, dim, device=get_device())\n",
    "    y = torch.randn(dim, dim, device=get_device())\n",
    "    # Return a function to perform the operation\n",
    "    return lambda : operation(x, y)\n",
    "\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # Warmup\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "        \n",
    "    # Run the code with the profiler\n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # Output stack trace for visualization\n",
    "            with_stack=with_stack,\n",
    "            # Needed to export stack trace for visualization\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "        \n",
    "    # Print out table\n",
    "    table = prof.key_averages().table(sort_by=\"cuda_time_total\",\n",
    "                                      max_name_column_width=80,\n",
    "                                      row_limit=10)\n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed80720d-94df-4e44-870d-3591d2801b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_function_128 = lambda a, b: a @ b\n",
    "matmul_profile_128 = profile(\"matmul(dim=128)\", run_operation2(dim=128, operation=matmul_function_128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7439a1b0-5868-4b7b-96e2-99dfdca51530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     aten::matmul        82.12%      29.066ms        99.92%      35.368ms      35.368ms       0.000us         0.00%       6.016us       6.016us             1  \n",
      "                                         aten::mm        17.52%       6.203ms        17.81%       6.303ms       6.303ms       6.016us       100.00%       6.016us       6.016us             1  \n",
      "                  ampere_sgemm_32x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us       6.016us       100.00%       6.016us       6.016us             1  \n",
      "    cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.08%      26.700us         0.08%      26.700us      26.700us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                 cudaLaunchKernel         0.21%      72.852us         0.21%      72.852us      72.852us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                            cudaDeviceSynchronize         0.08%      28.162us         0.08%      28.162us      14.081us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 35.397ms\n",
      "Self CUDA time total: 6.016us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(matmul_profile_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d30d3c-e8f0-4412-ade9-98b396c1db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu_function = lambda a, b: torch.nn.functional.gelu(a + b)\n",
    "gelu_profile = profile(\"gelu\", run_operation2(dim=2048, operation=gelu_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d1fb4b-2dbe-467d-ae47-3cedc57a5716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::add        92.90%       3.919ms        94.20%       3.973ms       3.973ms      60.704us        59.58%      60.704us      60.704us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      60.704us        59.58%      60.704us      60.704us             1  \n",
      "                                                                      aten::gelu         3.25%     136.899us         5.05%     212.891us     212.891us      41.184us        40.42%      41.184us      41.184us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::GeluCUDAKernelI...         0.00%       0.000us         0.00%       0.000us       0.000us      41.184us        40.42%      41.184us      41.184us             1  \n",
      "                                                                cudaLaunchKernel         3.10%     130.749us         3.10%     130.749us      65.374us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                                           cudaDeviceSynchronize         0.75%      31.769us         0.75%      31.769us      15.885us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.218ms\n",
      "Self CUDA time total: 101.888us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gelu_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2483496-b63e-4411-8680-03cac0ed00c8",
   "metadata": {},
   "source": [
    "# benchmarking_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958000c2-7b26-49ed-a3ae-e6828803bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ =  \"\"\"\n",
    "(a) Write a script to perform basic end-to-end benchmarking of the forward and backward passes in\n",
    "your model. Specifically, your script should support the following:\n",
    "• Given hyperparameters (e.g., number of layers), initialize a model.\n",
    "• Generate a random batch of data.\n",
    "• Run w warm-up steps (before you start measuring time), then time the execution of n steps\n",
    "(either only forward, or both forward and backward passes, depending on an argument). For\n",
    "timing, you can use the Python timeit module (e.g., either using the `timeit` function, or\n",
    "using `timeit.default_timer()`, which gives you the system's highest resolution clock, thus\n",
    "a better default for benchmarking than `time.time ()`).\n",
    "• Call torch. cuda.synchronize () after each step.\n",
    "Deliverable: A script that will initialize a basics Transformer model with the given hyperpa-\n",
    "rameters, create a random batch of data, and time forward and backward passes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147a154d-7ee2-4dd2-809d-4bf51c573ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs336_basics as lib\n",
    "import cs336_basics.model as nn\n",
    "import torch\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6aa1d47f-1a84-4f58-a28a-405b9c8a6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x: list[float]) -> float:\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # Warmup: first times might be slower due to compilation, things not cached.\n",
    "    # Since we will run the kernel multiple times, the timing that matters is steady state.\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "    # Time it for real now!\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "    for trial in range(num_trials):  # Do it multiple times to capture variance\n",
    "        start_time = time.time()\n",
    "        run()  # Actually perform computation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # @inspect mean_time\n",
    "    return mean_time\n",
    "\n",
    "def benchmark_model(\n",
    "    model,\n",
    "    batch_size: int = 1,\n",
    "    sequence_length: int = 64,\n",
    "    end_to_end: bool = False,\n",
    "    num_warmups: int = 1, \n",
    "    num_trials: int = 3\n",
    "):\n",
    "    # Initialization\n",
    "    vocab_size = model.vocab_size\n",
    "    X = torch.randint(high=vocab_size, size=(batch_size, sequence_length))\n",
    "    X = X.to(next(model.parameters()).device)\n",
    "    Y = torch.ones(vocab_size).to(next(model.parameters()).device)\n",
    "\n",
    "    def forward():\n",
    "        with torch.no_grad(): model(X)\n",
    "\n",
    "    def backward():\n",
    "        output = model(X)\n",
    "        loss = ((Y - output) ** 2).sum()\n",
    "        loss.backward()\n",
    "\n",
    "    run = backward if end_to_end else forward\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times: list[float] = []\n",
    "    for trial in range(num_trials):\n",
    "        start_time = timeit.default_timer()\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = timeit.default_timer()\n",
    "        times.append((end_time - start_time) * 1000)\n",
    "        \n",
    "    mean_time = mean(times)\n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4387df2-dd9f-4fda-bbe4-f0cbcde64225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration dictionaries\n",
    "configs = {\n",
    "    'small': {\n",
    "        'vocab_size': 50257,\n",
    "        'context_length': 2048,\n",
    "        'd_model': 768,\n",
    "        'num_layers': 12,\n",
    "        'num_heads': 12,\n",
    "        'd_ff': 3072,\n",
    "        'rope_theta': 10000.0,\n",
    "    },\n",
    "    \n",
    "    'medium': {\n",
    "        'vocab_size': 50257,\n",
    "        'context_length': 2048,\n",
    "        'd_model': 1024,\n",
    "        'num_layers': 24,\n",
    "        'num_heads': 16,\n",
    "        'd_ff': 4096,\n",
    "        'rope_theta': 10000.0,\n",
    "    },\n",
    "    \n",
    "    'large': {\n",
    "        'vocab_size': 50257,\n",
    "        'context_length': 2048,\n",
    "        'd_model': 1280,\n",
    "        'num_layers': 36,\n",
    "        'num_heads': 20,\n",
    "        'd_ff': 5120,\n",
    "        'rope_theta': 10000.0,\n",
    "    },\n",
    "    \n",
    "    'xl': {\n",
    "        'vocab_size': 50257,\n",
    "        'context_length': 2048,\n",
    "        'd_model': 1600,\n",
    "        'num_layers': 48,\n",
    "        'num_heads': 25,\n",
    "        'd_ff': 6400,\n",
    "        'rope_theta': 10000.0,\n",
    "    },\n",
    "    \n",
    "    '2.7B': {\n",
    "        'vocab_size': 50257,\n",
    "        'context_length': 2048,\n",
    "        'd_model': 2560,\n",
    "        'num_layers': 32,\n",
    "        'num_heads': 32,\n",
    "        'd_ff': 10240,\n",
    "        'rope_theta': 10000.0,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb05e80-2ca0-4fd4-baed-0ddc253e40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.BasicsTransformerLM(**configs[\"small\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5fb08df-457a-4762-ba53-4b0d3537cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ecccbe8-d81b-4c7c-96a8-d722fca60449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.16990557685494"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_model(\n",
    "    model,\n",
    "    batch_size=5,\n",
    "    sequence_length=32,\n",
    "    end_to_end=True,\n",
    "    num_warmups=5,\n",
    "    num_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba0ade-0d78-4bd0-8fe3-2501ecaf2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "for warmups in 0 5; do\n",
    "    for end_to_end in \"\" \"--end-to-end\"; do\n",
    "        echo \"Running: warmups=$warmups, end_to_end=${end_to_end:-False}\"\n",
    "        \n",
    "        python cs336_systems/measure.py \\\n",
    "            --num-warmups $warmups \\\n",
    "            $end_to_end \\\n",
    "            --quiet \\\n",
    "            --log-level INFO\n",
    "    done\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
